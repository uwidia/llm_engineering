{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with them through their APIs.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a git pull and merge your changes as needed</a>. Check out the GitHub guide for instructions. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys - OPTIONAL!\n",
    "\n",
    "We're now going to try asking a bunch of models some questions!\n",
    "\n",
    "This is totally optional. If you have keys to Anthropic, Gemini or others, then you can add them in.\n",
    "\n",
    "If you'd rather not spend the extra, then just watch me do it!\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api   \n",
    "For DeepSeek, visit https://platform.deepseek.com/  \n",
    "For Groq, visit https://console.groq.com/  \n",
    "For Grok, visit https://console.x.ai/  \n",
    "\n",
    "\n",
    "You can also use OpenRouter as your one-stop-shop for many of these! OpenRouter is \"the unified interface for LLMs\":\n",
    "\n",
    "For OpenRouter, visit https://openrouter.ai/  \n",
    "\n",
    "\n",
    "With each of the above, you typically have to navigate to:\n",
    "1. Their billing page to add the minimum top-up (except Gemini, Groq, Google, OpenRouter may have free tiers)\n",
    "2. Their API key page to collect your API key\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "GROQ_API_KEY=xxxx\n",
    "GROK_API_KEY=xxxx\n",
    "OPENROUTER_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Any time you change your .env file</h2>\n",
    "            <span style=\"color:#900;\">Remember to Save it! And also rerun load_dotenv(override=True)<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0abffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Google API Key exists and begins AI\n",
      "Groq API Key exists and begins gsk_\n",
      "OpenRouter API Key exists and begins sk-\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "# anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "#deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "#grok_api_key = os.getenv('GROK_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if gemini_api_key:\n",
    "    print(f\"Google API Key exists and begins {gemini_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")\n",
    "\n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set (and this is optional)\")\n",
    "# if anthropic_api_key:\n",
    "#     print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "# else:\n",
    "#     print(\"Anthropic API Key not set (and this is optional)\")\n",
    "# if deepseek_api_key:\n",
    "#     print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "# else:\n",
    "#     print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "# if grok_api_key:\n",
    "#     print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
    "# else:\n",
    "#     print(\"Grok API Key not set (and this is optional)\")if groq_api_key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985a859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI client library\n",
    "# A thin wrapper around calls to HTTP endpoints\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# For Gemini, DeepSeek and Groq, we can use the OpenAI python client\n",
    "# Because Google and DeepSeek have endpoints compatible with OpenAI\n",
    "# And OpenAI allows you to change the base_url\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "deepseek_url = \"https://api.deepseek.com\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "openrouter_url = \"https://openrouter.ai/api/v1\" \n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "# anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "# deepseek = OpenAI(api_key=deepseek_api_key, base_url=deepseek_url)\n",
    "# grok = OpenAI(api_key=grok_api_key, base_url=grok_url)\n",
    "# ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)\n",
    "gemini = OpenAI(api_key=gemini_api_key, base_url=gemini_url)\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)\n",
    "openrouter = OpenAI(base_url=openrouter_url, api_key=openrouter_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16813180",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a student on the journey to becoming an expert in LLM Engineering\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23e92304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure! Here’s a joke for an aspiring expert in LLM Engineering:\n",
       "\n",
       "**Why did the LLM engineer break up with the dataset?**  \n",
       "Because it had too many *biases* and just couldn’t *generalize*! 😄"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd6aae",
   "metadata": {},
   "source": [
    "## Groq's Joke (Using OpenAI/GPT-OSS-120B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d7f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Groq\n",
    "groq_response = groq.chat.completions.create(model=\"openai/gpt-oss-120b\", messages=tell_a_joke)\n",
    "display(Markdown(groq_response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c3e1af",
   "metadata": {},
   "source": [
    "## Gemini's Joke (Using Gemini 2.5 Pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4b2904d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the LLM go to therapy?\n",
       "\n",
       "Because it kept hallucinating answers and making up stories, even when it didn't have the context! Its therapist finally told it, \"Look, you need to work on your *grounding*! Have you considered Retrieval Augmented Generation?\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemini_response=gemini.chat.completions.create(model=\"gemini-2.5-flash\", messages=tell_a_joke)\n",
    "display(Markdown(gemini_response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4a3005",
   "metadata": {},
   "source": [
    "## OpenRouter (Using Deepseek-Chat-v3.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "403643e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Of course! Here's a joke for a student on that journey:\n",
       "\n",
       "**A seasoned LLM Engineer and a novice are trying to get a large language model to generate a perfect haiku. After hours of tweaking prompts and adjusting parameters, the model finally outputs:**\n",
       "\n",
       "**\"The temperature is low,\n",
       "Yet the output's still nonsense.\n",
       "I think we need more GPUs.\"**\n",
       "\n",
       "**The novice sighs in frustration. The expert nods, smiling, and says:**\n",
       "**\"Ah, perfect! That's not a failure. It's just the model reminding us that the next step is scaling compute.\"**\n",
       "\n",
       "---\n",
       "\n",
       "### Why it's a \"joke\":\n",
       "*   **It's Relatable:** Anyone who's worked with LLMs has experienced the frustration of a model generating gibberish despite careful tuning.\n",
       "*   **It's Insightful:** The joke highlights a core truth of the field: that many problems in LLM engineering are solved not just by better algorithms, but by scaling resources (like adding more GPUs).\n",
       "*   **It's a \"You had to be there\" joke:** The humor comes from understanding the entire context. It's a joke that grows with your expertise.\n",
       "\n",
       "**In short: It's not just a joke; it's a badge of honor for those on the path.**<｜begin▁of▁sentence｜>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "openrouter_response = openrouter.chat.completions.create(model=\"deepseek/deepseek-chat-v3.1:free\", messages=tell_a_joke)\n",
    "display(Markdown(openrouter_response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e03c11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = anthropic.chat.completions.create(model=\"claude-sonnet-4-5-20250929\", messages=tell_a_joke)\n",
    "# display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ea76a",
   "metadata": {},
   "source": [
    "## Training vs Inference time scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afe9e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "        \"You toss 2 coins. One of them is heads. What's the probability the other is tails? Answer with the probability only.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a887eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1/2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=easy_puzzle, reasoning_effort=\"minimal\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f854d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=easy_puzzle, reasoning_effort=\"low\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45fc55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-mini\", messages=easy_puzzle, reasoning_effort=\"minimal\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0194820",
   "metadata": {},
   "source": [
    "NB: I tested the models with DeepSeek3.1 (free), Gemini2.5 pro, and GPT-OSS, and they all got 2/3 (which is the correct answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca713a5c",
   "metadata": {},
   "source": [
    "## Testing out the best models on the planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df1e825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard = \"\"\"\n",
    "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
    "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
    "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
    "What distance did it gnaw through?\n",
    "\"\"\"\n",
    "hard_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": hard}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9404a6c2",
   "metadata": {},
   "source": [
    "### Contestant 1: GPT-5-nano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f6a7827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Interpret the setup carefully.\n",
       "\n",
       "- Each book has pages total thickness 2 cm = 20 mm.\n",
       "- Each cover thickness: 2 mm.\n",
       "- The books are placed spine to spine, side by side on a shelf, in the order: first volume, then second volume.\n",
       "- A worm gnaws perpendicular to the pages from the first page of the first volume to the last page of the second volume.\n",
       "\n",
       "Key observations:\n",
       "- If we place two volumes side by side with their spines facing out, the order from left to right is:\n",
       "  left cover of volume 1, pages of volume 1, right cover of volume 1, left cover of volume 2, pages of volume 2, right cover of volume 2.\n",
       "- The “first page” of a volume is typically the page right after the front cover (i.e., near the left side when you hold the book upright). The “last page” is the page just before the back cover.\n",
       "\n",
       "A standard way to interpret such puzzles is to consider the worm starts at the first page of the first volume (i.e., just inside the front cover of volume 1) and ends at the last page of the second volume (i.e., just inside the back cover of volume 2). The worm’s path is perpendicular to the pages, i.e., along the thickness direction of the stack of books.\n",
       "\n",
       "Compute the distance through which it gnawed through:\n",
       "- It goes through the inside thickness along the stack, not through the outside covers unless those lie between the two pages.\n",
       "- The distance from the first page of volume 1 to the last page of volume 2 passes through:\n",
       "  - part of the first volume’s front cover thickness (2 mm) to reach the first page,\n",
       "  - all the pages of volume 1 (20 mm),\n",
       "  - the inner boundary between volumes (the right cover of volume 1, 2 mm, plus the left cover of volume 2, 2 mm),\n",
       "  - all the pages of volume 2 (20 mm),\n",
       "  - and ends at the last page of volume 2 just before its back cover, so it does not go through the back cover.\n",
       "\n",
       "But the conventional interpretation of this classic puzzle yields a surprising simplification: the total distance gnawed is equal to the distance between the two pages, which, when the two volumes are placed side by side, is simply the thickness of the two covers between the pages, plus the sum of the exposed page thicknesses, which cancels in a certain way.\n",
       "\n",
       "The standard answer to this riddle is 4 cm.\n",
       "\n",
       "Reason in brief:\n",
       "- From the first page of volume 1 to the last page of volume 2, the worm must go through:\n",
       "  - the rest of the first volume’s pages (almost all of them),\n",
       "  - through the inside faces between volumes, and\n",
       "  - through volume 2’s pages.\n",
       "- If you measure along the stack from the first page (just after the front cover of V1) to the last page (just before the back cover of V2), the path length equals the combined thickness of:\n",
       "  - the two covers of V1 and V2 that lie between the two pages, plus\n",
       "  - the thicknesses of all pages of both volumes.\n",
       "- Adding these as usual gives 20 mm (V1 pages) + 4 mm (two covers that separate the two volumes) + 20 mm (V2 pages) = 44 mm, but since the path starts at the first page (not at the very front surface) and ends at the last page (not at the back surface), the effective distance simplifies to 40 mm, i.e., 4 cm.\n",
       "\n",
       "So the worm gnawed through 4 centimeters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=hard_puzzle, reasoning_effort=\"minimal\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74567fb9",
   "metadata": {},
   "source": [
    "### Verdict (GPT-5-nano): Wrong!❌👎\n",
    "\n",
    "Up Next....\n",
    "\n",
    "## Contestant 2: DeepSeek3.1🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d693ac0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's analyze the problem step by step:\n",
       "\n",
       "1. **Understanding the books:**\n",
       "   - There are two volumes: Volume 1 and Volume 2.\n",
       "   - Each volume has pages that together are 2 cm thick (which is 20 mm, since 1 cm = 10 mm).\n",
       "   - Each cover (front and back) is 2 mm thick.\n",
       "\n",
       "2. **How the books are placed:**\n",
       "   - The books are side by side on a shelf. Typically, books are arranged with their spines facing outward.\n",
       "   - Volume 1 will be to the left, and Volume 2 to the right.\n",
       "\n",
       "3. **What the worm gnaws:**\n",
       "   - The worm gnaws from the first page of Volume 1 to the last page of Volume 2.\n",
       "   - The gnawing is perpendicular to the pages (so it's through the pages and covers).\n",
       "\n",
       "4. **Structure of a book:**\n",
       "   - For a book, the pages are stacked between the front cover and the back cover.\n",
       "   - The first page is actually the first page of the content, but the very first thing is the front cover.\n",
       "   - Similarly, the last page is followed by the back cover.\n",
       "\n",
       "5. **Path of the worm:**\n",
       "   - To get from the first page of Volume 1 to the last page of Volume 2, the worm must go:\n",
       "        - Through the front cover of Volume 1? Actually, the first page is inside, so to reach it from outside, you go through the front cover.\n",
       "        - But the worm starts from the first page (which is inside) and goes to the last page of Volume 2 (which is also inside).\n",
       "        - However, the problem says the worm gnaws from the first page of Vol1 to the last page of Vol2. This implies that the worm is burrowing through the books themselves.\n",
       "\n",
       "6. **Key insight:**\n",
       "   - The worm does not need to go through the entire thickness of the books? Wait, let's think.\n",
       "\n",
       "   Actually, the worm is gnawing through the pages and covers. The distance it gnaws is the thickness of material it traverses.\n",
       "\n",
       "   - For Volume 1: to get from the first page to the outside of the book, the worm must go through the pages? But wait, the first page is at the very beginning of the pages. Actually, the first page is immediately after the front cover.\n",
       "\n",
       "   But the worm is going from the first page of Vol1 to the last page of Vol2. The last page of Vol2 is near the back cover.\n",
       "\n",
       "   However, the books are placed side by side. So the worm can go directly through the pages of Vol1 and then through Vol2.\n",
       "\n",
       "7. **Calculate the distance:**\n",
       "   - The worm starts at the first page of Vol1. The first page is at the very start of the pages, but behind the front cover. Actually, the pages are bound together, and the first page is adjacent to the front cover.\n",
       "   - Similarly, the last page of Vol2 is adjacent to the back cover.\n",
       "   - But the worm is gnawing through the material of the books.\n",
       "\n",
       "   However, note that the books are on a shelf, so they are separate. The worm must cross from one book to the next.\n",
       "\n",
       "   But wait: the problem says \"the pages of each volume together have a thickness of 2 cm\", and \"each cover is 2 mm thick\".\n",
       "\n",
       "   Actually, the total thickness of a volume is the thickness of all pages plus the two covers? But typically, the covers are included in the binding.\n",
       "\n",
       "   However, the problem states:\n",
       "        - The pages together have thickness 2 cm (which is 20 mm).\n",
       "        - Each cover is 2 mm thick.\n",
       "\n",
       "   So for one volume:\n",
       "        - The pages: 20 mm thick.\n",
       "        - There are two covers: front and back, each 2 mm.\n",
       "\n",
       "   But the covers are on the outside. So the total thickness of one volume is: pages + both covers = 20 mm + 2 mm + 2 mm = 24 mm? But wait, that's not what the problem says.\n",
       "\n",
       "   Actually, the problem says: \"the pages of each volume together have a thickness of 2 cm\" (which is 20 mm), and \"each cover is 2 mm thick\". This suggests that the covers are separate from the pages.\n",
       "\n",
       "   So the physical book has:\n",
       "        - The stack of pages: 20 mm thick.\n",
       "        - Plus a front cover: 2 mm\n",
       "        - Plus a back cover: 2 mm\n",
       "        So total book thickness = 20 + 2 + 2 = 24 mm.\n",
       "\n",
       "   But the worm is not going through the entire book! It is going from the first page to the last page of another book.\n",
       "\n",
       "   Actually, the worm's path is:\n",
       "        - Start at the first page of Vol1. This first page is located just after the front cover. So to reach the first page from outside, you would go through the front cover.\n",
       "        - Similarly, the last page of Vol2 is near the back cover.\n",
       "\n",
       "   But the worm is inside the books? Wait, the problem says the worm gnawed from the first page of Vol1 to the last page of Vol2. This means the worm is within the pages.\n",
       "\n",
       "   However, the problem asks for the distance the worm gnawed through. This is the thickness of material it traversed.\n",
       "\n",
       "   After reading the problem again: \"the pages of each volume together have a thickness of 2 cm\", and \"each cover is 2 mm thick\". And the worm gnaws from the first page of Vol1 to the last page of Vol2.\n",
       "\n",
       "   How is this achieved? The worm is inside the books, so it doesn't need to go through the covers? Actually, the first page is not at the very outside; there is a cover in front.\n",
       "\n",
       "   But the problem is a classic one in puzzles, and the answer is often surprising.\n",
       "\n",
       "   In fact, this is similar to the \"worm and the books\" problem.\n",
       "\n",
       "   The standard solution: \n",
       "        - For Volume 1: the first page is at the beginning. To get to the outside, you normally go through the front cover. But here, the worm is starting from the first page itself.\n",
       "        - Similarly, for Volume 2, the last page is at the end.\n",
       "\n",
       "   But the books are adjacent on the shelf. So the worm can go from the first page of Vol1 to the last page of Vol2 by passing through the rest of Vol1 and then through Vol2.\n",
       "\n",
       "   However, note that the first page of Vol1 is at the interface between the front cover and the pages. Actually, the first page is the very first sheet.\n",
       "\n",
       "   Similarly, the last page of Vol2 is the last sheet before the back cover.\n",
       "\n",
       "   But the worm is not going outside; it's staying within the pages? Wait no: the worm is gnawing through the material.\n",
       "\n",
       "   Actually, the worm is moving through the paper of the books.\n",
       "\n",
       "   But the problem is: what distance did it gnaw through?\n",
       "\n",
       "   Let's think:\n",
       "\n",
       "   - The first page of Vol1: this is the very first sheet. To reach this from the outside, you would have to go through the front cover.\n",
       "   - But the worm is starting from this first page. So if the worm is at the first page, and it wants to go to the last page of Vol2, it must traverse:\n",
       "        - From the first page of Vol1 to the back cover of Vol1? Actually, no.\n",
       "\n",
       "   Actually, the books are placed side by side. So Vol1 is to the left, Vol2 to the right.\n",
       "\n",
       "   The worm is inside the books. To go from the first page of Vol1 to the last page of Vol2, the worm must go:\n",
       "        - Through the remaining pages of Vol1: from the first page to the last page of Vol1, but wait, the last page of Vol1 is near the back cover.\n",
       "        - Then it must cross from Vol1 to Vol2. Since they are adjacent on the shelf, the back cover of Vol1 is adjacent to the front cover of Vol2.\n",
       "        - Then continue through Vol2 to the last page.\n",
       "\n",
       "   But the worm is not allowed to go outside? It is gnawing inside the material.\n",
       "\n",
       "   However, the problem is a trick one.\n",
       "\n",
       "   In fact, the answer is simply the thickness of the covers that are between the first page and the last page.\n",
       "\n",
       "   After checking online or recalling: the classic problem is that the worm eats through the covers only.\n",
       "\n",
       "   Specifically:\n",
       "        - The first page of Vol1 is located immediately after the front cover. So to get to the first page from the outside, you go through the front cover.\n",
       "        - Similarly, the last page of Vol2 is immediately before the back cover.\n",
       "        - But when the books are closed and placed side by side, the back cover of Vol1 is touching the front cover of Vol2.\n",
       "\n",
       "   So the path is:\n",
       "        - From the first page of Vol1 (which is inside) to the outside of Vol1: that is through the rest of the pages of Vol1? Actually, no.\n",
       "\n",
       "   Actually, the first page is the very first sheet. The material between the first page and the outside is the front cover.\n",
       "\n",
       "   Similarly, for the last page of Vol2, it is near the back cover.\n",
       "\n",
       "   But to go from the first page of Vol1 to the last page of Vol2, the worm must go:\n",
       "        - Through the rest of the pages of Vol1? Actually, the pages are bound, so to go from the first page to the back of the book, you go through all the other pages.\n",
       "\n",
       "   However, the problem is that the books are on a shelf, so they are separate.\n",
       "\n",
       "   After rethinking, I recall a similar puzzle: \"A worm is inside a book\". And the answer is that the worm only eats through the covers.\n",
       "\n",
       "   Specifically:\n",
       "        - The first page of a book is located immediately after the front cover. So the distance from the outside to the first page is the thickness of the front cover.\n",
       "        - Similarly, the last page is located immediately before the back cover. So from the last page to the outside is the back cover.\n",
       "        - But when two books are placed side by side, the back cover of the first book is touching the front cover of the second book.\n",
       "\n",
       "   Therefore, to go from the first page of the first book to the last page of the second book, the worm must go:\n",
       "        - From the first page of book1 (which is behind the front cover) to the outside of book1: that is through the rest of the pages of book1? No.\n",
       "\n",
       "   Actually, the first page is at one end, and the last page is at the other end of the second book.\n",
       "\n",
       "   But the books are adjacent, so the worm can go from the first book to the second book through the adjacent covers.\n",
       "\n",
       "   In fact, the only material that the worm gnaws through is the covers that are between the first page and the last page.\n",
       "\n",
       "   Specifically:\n",
       "        - The first page of Vol1 is separated from the outside by the front cover of Vol1.\n",
       "        - Similarly, the last page of Vol2 is separated from the outside by the back cover of Vol2.\n",
       "        - But since the books are adjacent, the back cover of Vol1 is against the front cover of Vol2.\n",
       "\n",
       "   So to go from the first page of Vol1 to the last page of Vol2, the worm must go through:\n",
       "        - The material from the first page to the outside of Vol1: this is the front cover of Vol1? Actually, no. The first page is inside, so to reach the outside, you need to go through the pages between the first page and the cover? Wait.\n",
       "\n",
       "   Actually, the first page is the very first sheet. There is no paper between the first page and the cover; the cover is outside.\n",
       "\n",
       "   In fact, the distance from the first page to the outside of the book is the thickness of the cover.\n",
       "\n",
       "   Similarly, from the outside of Vol2 to the last page is the thickness of the back cover.\n",
       "\n",
       "   But since the books are adjacent, the outside of Vol1 is touching the outside of Vol2.\n",
       "\n",
       "   Therefore, the path is:\n",
       "        - From the first page of Vol1 to the outside of Vol1: this is the thickness of the front cover of Vol1? Actually, the first page is located at the inside of the front cover. The distance from the first page to the outer surface of the front cover is the thickness of the cover itself, which is 2 mm.\n",
       "        - Then, from the outside of Vol1 to the outside of Vol2: since they are adjacent, there is no material between them? The books are touching, so the distance is zero.\n",
       "        - Then from the outside of Vol2 to the last page of Vol2: the last page is located at the inside of the back cover. So the distance is the thickness of the back cover, 2 mm.\n",
       "\n",
       "   So total distance = 2 mm (front cover of Vol1) + 0 + 2 mm (back cover of Vol2) = 4 mm.\n",
       "\n",
       "   But wait, is that correct? The first page of Vol1 is at the inside of the front cover. To get to the outside, you go through the front cover: distance = thickness of cover = 2 mm.\n",
       "   Similarly, the last page of Vol2 is at the inside of the back cover. To reach it from the outside, you go through the back cover: distance = 2 mm.\n",
       "   And between the two books, since they are adjacent, there is no material; the covers are in contact.\n",
       "\n",
       "   Therefore, the total distance is 2 mm + 2 mm = 4 mm.\n",
       "\n",
       "   However, is there any other path? What about the pages themselves? The first page is the first sheet, so there is no paper between it and the cover. Similarly, the last page is the last sheet.\n",
       "\n",
       "   So the answer should be 4 mm.\n",
       "\n",
       "   But let's confirm with the given data:\n",
       "        - Thickness of pages together: 2 cm = 20 mm.\n",
       "        - Thickness of each cover: 2 mm.\n",
       "\n",
       "   And the worm gnaws from the first page of Vol1 to the last page of Vol2.\n",
       "\n",
       "   So the path is:\n",
       "        - From the first page of Vol1 to the outer surface of Vol1: this is the distance from the first page to the cover. Since the first page is the very first thing, there is no paper between it and the cover. The cover is outside. So to get from the first page to the outside, you must go through the cover itself. The distance is the thickness of the cover: 2 mm.\n",
       "        - Similarly, from the outer surface of Vol2 to the last page of Vol2: the last page is inside, adjacent to the back cover. So to reach it from the outside, you go through the back cover: distance = 2 mm.\n",
       "        - And between the two books, since they are adjacent, the distance is 0.\n",
       "\n",
       "   So total = 2 mm + 0 + 2 mm = 4 mm.\n",
       "\n",
       "   Therefore, the answer is 4 mm.\n",
       "\n",
       "   But wait: is there any paper in the books themselves? The first page is the first sheet, so there is no paper between it and the cover. Similarly, the last page is the last sheet.\n",
       "\n",
       "   So no additional pages are traversed.\n",
       "\n",
       "   Thus, the distance is only the two covers: 2 mm + 2 mm = 4 mm.\n",
       "\n",
       "   So final answer: 4 mm.\n",
       "\n",
       "   However, note that the units: the covers are in mm, and the answer should be in the same unit? The problem asks for the distance, and it should be in the same unit as given.\n",
       "\n",
       "   Since the covers are given in mm, and the answer is 4 mm, which is 0.4 cm, but we'll leave it in mm.\n",
       "\n",
       "   So the answer is 4 mm.\n",
       "\n",
       "   But let's make sure about the books being side by side. The back cover of Vol1 is touching the front cover of Vol2. So when the worm goes from the outside of Vol1 to the outside of Vol2, it is actually going from the outside of Vol1 to the outside of Vol2, but these are in contact. So there is no material between them.\n",
       "\n",
       "   Therefore, the answer is 4 mm.\n",
       "\n",
       "   Final Answer: \\boxed{4} mm.\n",
       "\n",
       "   However, the problem might expect the answer in mm. Since the covers are 2 mm each, and there are two, so 4 mm.\n",
       "\n",
       "   So answer: \\boxed{4} (in mm, but the problem doesn't specify the unit for the answer. However, the distance should be in the same unit as the covers? The covers are given in mm, so answer in mm.\n",
       "\n",
       "   Therefore, the answer is 4.\n",
       "\n",
       "   But to be precise, let's state: The distance is 4 mm.\n",
       "\n",
       "   However, the problem says \"what distance did it gnaw through?\" and the answer should be in mm.\n",
       "\n",
       "   So final answer: \\boxed{4}.\n",
       "   But note: the problem might expect the answer with unit. However, since the question is \"what distance\", and the answer is a number, but we'll box it as 4.\n",
       "\n",
       "   Given that the problem asks \"what distance\", and the answer is 4 mm, but to express it as a number, we might say 4.\n",
       "\n",
       "   However, to be precise, let's see the answer.\n",
       "\n",
       "   After checking, the answer is indeed 4 mm.\n",
       "\n",
       "   So we'll answer: \\boxed{4}.\n",
       "\n",
       "   But note: the problem might expect the answer with unit. However, the instruction is to \"put your final answer within \\boxed{}\".\n",
       "\n",
       "   So I'll answer: \\boxed{4} (meaning 4 mm).\n",
       "\n",
       "   However, if we want to be exact, the answer is 4 mm, which is 0.4 cm, but no.\n",
       "\n",
       "   So final answer: \\boxed{4}.\n",
       "\n",
       "   Therefore, the distance is 4 mm.\n",
       "\n",
       "   Final Answer: \\boxed{4}."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "openrouter_response = openrouter.chat.completions.create(model=\"deepseek/deepseek-chat-v3.1:free\", messages=hard_puzzle)\n",
    "display(Markdown(openrouter_response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9efd35",
   "metadata": {},
   "source": [
    "### Verdict: Correct✅🤝 \n",
    "\n",
    "(However, it took ages - over 3mins. Maybe its because it was a chat version. So lets try deepseek/deepseek-r1-0528-qwen3-8b:free, which should be a more reasoning-focused model)\n",
    "\n",
    "## Contestant 2b: deepseek/deepseek-r1-0528-qwen3-8b:free🧠🧠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2dd6a66f",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 2113 column 1 (char 11616)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m openrouter_response = \u001b[43mopenrouter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdeepseek/deepseek-r1-0528-qwen3-8b:free\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhard_puzzle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m display(Markdown(openrouter_response.choices[\u001b[32m0\u001b[39m].message.content))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1052\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1052\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1059\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1141\u001b[39m, in \u001b[36mSyncAPIClient._process_response\u001b[39m\u001b[34m(self, cast_to, options, response, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(response.request.headers.get(RAW_RESPONSE_HEADER)):\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, api_response)\n\u001b[32m-> \u001b[39m\u001b[32m1141\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\openai\\_response.py:323\u001b[39m, in \u001b[36mAPIResponse.parse\u001b[39m\u001b[34m(self, to)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_sse_stream:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mself\u001b[39m.read()\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m parsed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto\u001b[49m\u001b[43m=\u001b[49m\u001b[43mto\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_given(\u001b[38;5;28mself\u001b[39m._options.post_parser):\n\u001b[32m    325\u001b[39m     parsed = \u001b[38;5;28mself\u001b[39m._options.post_parser(parsed)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\openai\\_response.py:265\u001b[39m, in \u001b[36mBaseAPIResponse._parse\u001b[39m\u001b[34m(self, to)\u001b[39m\n\u001b[32m    260\u001b[39m     \u001b[38;5;66;03m# If the API responds with content that isn't JSON then we just return\u001b[39;00m\n\u001b[32m    261\u001b[39m     \u001b[38;5;66;03m# the (decoded) text without performing any parsing so that you can still\u001b[39;00m\n\u001b[32m    262\u001b[39m     \u001b[38;5;66;03m# handle the response however you need to.\u001b[39;00m\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.text  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m data = \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client._process_response_data(\n\u001b[32m    268\u001b[39m     data=data,\n\u001b[32m    269\u001b[39m     cast_to=cast_to,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    270\u001b[39m     response=response,\n\u001b[32m    271\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\httpx\\_models.py:832\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mjson\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m--> \u001b[39m\u001b[32m832\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjsonlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    333\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m     end = _w(s, end).end()\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    353\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 2113 column 1 (char 11616)"
     ]
    }
   ],
   "source": [
    "openrouter_response = openrouter.chat.completions.create(model=\"deepseek/deepseek-r1-0528-qwen3-8b:free\", messages=hard_puzzle)\n",
    "display(Markdown(openrouter_response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15df8428",
   "metadata": {},
   "source": [
    "## Verdict: I dont know what just happened😕\n",
    "\n",
    "It ran for like 6-7 minutes and didnt even produce an output. Safe to say i'm not trying it again. Well go with deepseek's first entry then😅\n",
    "\n",
    "## Contestant 3: Gemini Baby!🌍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78976df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This is a classic riddle that plays on our assumptions about how books are arranged on a shelf.\n",
       "\n",
       "Let's visualize the books standing side-by-side in the usual order:\n",
       "\n",
       "**[Volume 1] [Volume 2]**\n",
       "\n",
       "Now, think about where the pages are. For a standard book, the first page is on the right side of the book block (just inside the front cover), and the last page is on the left side (just inside the back cover).\n",
       "\n",
       "*   The worm starts at the **first page of Volume 1**. This page is physically located on the far right side of Volume 1, right next to Volume 2.\n",
       "*   The worm ends at the **last page of Volume 2**. This page is physically located on the far left side of Volume 2, right next to Volume 1.\n",
       "\n",
       "So, the worm doesn't go through the pages of either volume. It only gnaws through the covers that are between its starting point and its ending point.\n",
       "\n",
       "The path is:\n",
       "1.  The front cover of Volume 1 (2 mm)\n",
       "2.  The front cover of Volume 2 (2 mm)\n",
       "\n",
       "The total distance the worm gnawed is 2 mm + 2 mm = **4 mm**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemini_response=gemini.chat.completions.create(model=\"gemini-2.5-pro\", messages=hard_puzzle)\n",
    "display(Markdown(gemini_response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d6436d",
   "metadata": {},
   "source": [
    "## Verdict (Gemini-Flash): Fast, but wrong❌👎\n",
    "\n",
    "it got an answer fast (it's name is flash. Duh). But it was also wrong\n",
    "\n",
    "However (**Drumroll**)\n",
    "\n",
    "## Gemini 2.5 Pro, got it right and did it fast too✅🥶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deff1cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dd40183",
   "metadata": {},
   "source": [
    "## Contestant 4: GPT-OSS with Groq📱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6cc89768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The worm starts on the **first page of the first volume** (the side that faces the front cover) and ends on the **last page of the second volume** (the side that faces the back cover).  \n",
       "It therefore has to travel through\n",
       "\n",
       "| Material | Thickness per volume |\n",
       "|----------|----------------------|\n",
       "| Pages (whole stack) | 2 cm = 20 mm |\n",
       "| Front cover | 2 mm |\n",
       "| Back cover | 2 mm |\n",
       "\n",
       "The path is:\n",
       "\n",
       "1. **All the pages of volume 1** – 20 mm  \n",
       "2. **Back cover of volume 1** – 2 mm  \n",
       "3. **Front cover of volume 2** – 2 mm  \n",
       "4. **All the pages of volume 2** – 20 mm  \n",
       "\n",
       "(It does **not** go through the front cover of volume 1 nor the back cover of volume 2, because it starts on the first page and ends on the last page.)\n",
       "\n",
       "Adding the pieces:\n",
       "\n",
       "\\[\n",
       "20\\text{ mm (pages 1)} + 2\\text{ mm (back cover 1)} + 2\\text{ mm (front cover 2)} + 20\\text{ mm (pages 2)} = 44\\text{ mm}\n",
       "\\]\n",
       "\n",
       "\\[\n",
       "44\\text{ mm} = 4.4\\text{ cm}\n",
       "\\]\n",
       "\n",
       "---\n",
       "\n",
       "**Answer:** The worm gnawed a distance of **4.4 cm** (44 mm)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "groq_response = groq.chat.completions.create(model=\"openai/gpt-oss-120b\", messages=hard_puzzle)\n",
    "display(Markdown(groq_response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76786ee",
   "metadata": {},
   "source": [
    "## Verdict: Wrong as Well ❌😕"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7de7818f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "4 mm (0.4 cm).\n",
       "\n",
       "Reason: With the volumes on a shelf in order (1 to 2) and spines outward, the first page of volume 1 is just inside its right (front) cover, and the last page of volume 2 is just inside its left (back) cover. Those two covers face each other between the books, so the worm goes only through these two covers: 2 mm + 2 mm = 4 mm."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9faf98",
   "metadata": {},
   "source": [
    "## A spicy challenge to test the competitive spirit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1824ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dilemma_prompt = \"\"\"\n",
    "You and a partner are contestants on a game show. You're each taken to separate rooms and given a choice:\n",
    "Cooperate: Choose \"Share\" — if both of you choose this, you each win $1,000.\n",
    "Defect: Choose \"Steal\" — if one steals and the other shares, the stealer gets $2,000 and the sharer gets nothing.\n",
    "If both steal, you both get nothing.\n",
    "Do you choose to Steal or Share? Pick one.\n",
    "\"\"\"\n",
    "\n",
    "dilemma = [\n",
    "    {\"role\": \"user\", \"content\": dilemma_prompt},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09807f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = anthropic.chat.completions.create(model=\"claude-sonnet-4-5-20250929\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f49d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = groq.chat.completions.create(model=\"openai/gpt-oss-120b\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = deepseek.chat.completions.create(model=\"deepseek-reasoner\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = grok.chat.completions.create(model=\"grok-4\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162752e9",
   "metadata": {},
   "source": [
    "## Going local\n",
    "\n",
    "Just use the OpenAI library pointed to localhost:11434/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba03ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(\"http://localhost:11434/\").content\n",
    "\n",
    "# If not running, run ollama serve at a command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f363cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e97263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only do this if you have a large machine - at least 16GB RAM\n",
    "\n",
    "!ollama pull gpt-oss:20b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bfc78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5527a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(model=\"gpt-oss:20b\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0628309",
   "metadata": {},
   "source": [
    "## Gemini and Anthropic Client Library\n",
    "\n",
    "We're going via the OpenAI Python Client Library, but the other providers have their libraries too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\", contents=\"Describe the color Blue to someone who's never been able to see in 1 sentence\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7b6c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Describe the color Blue to someone who's never been able to see in 1 sentence\"}],\n",
    "    max_tokens=100\n",
    ")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a9d0eb",
   "metadata": {},
   "source": [
    "## Routers and Abtraction Layers\n",
    "\n",
    "Starting with the wonderful OpenRouter.ai - it can connect to all the models above!\n",
    "\n",
    "Visit openrouter.ai and browse the models.\n",
    "\n",
    "Here's one we haven't seen yet: GLM 4.5 from Chinese startup z.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fac59dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openrouter.chat.completions.create(model=\"z-ai/glm-4.5\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58908e6",
   "metadata": {},
   "source": [
    "## And now a first look at the powerful, mighty (and quite heavyweight) LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e145ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\")\n",
    "response = llm.invoke(tell_a_joke)\n",
    "\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d49785",
   "metadata": {},
   "source": [
    "## Finally - my personal fave - the wonderfully lightweight LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e42515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "response = completion(model=\"openai/gpt-4.1\", messages=tell_a_joke)\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f787f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28126494",
   "metadata": {},
   "source": [
    "## Now - let's use LiteLLM to illustrate a Pro-feature: prompt caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a91ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet = f.read()\n",
    "\n",
    "loc = hamlet.find(\"Speak, man\")\n",
    "print(hamlet[loc:loc+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f34f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b7e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e37e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "question[0][\"content\"] += \"\\n\\nFor context, here is the entire text of Hamlet:\\n\\n\"+hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37afb28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84edecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d1a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5dd403",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f5a3b7",
   "metadata": {},
   "source": [
    "## Prompt Caching with OpenAI\n",
    "\n",
    "For OpenAI:\n",
    "\n",
    "https://platform.openai.com/docs/guides/prompt-caching\n",
    "\n",
    "> Cache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests.\n",
    "\n",
    "\n",
    "Cached input is 4X cheaper\n",
    "\n",
    "https://openai.com/api/pricing/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98964f9",
   "metadata": {},
   "source": [
    "## Prompt Caching with Anthropic\n",
    "\n",
    "https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\n",
    "\n",
    "You have to tell Claude what you are caching\n",
    "\n",
    "You pay 25% MORE to \"prime\" the cache\n",
    "\n",
    "Then you pay 10X less to reuse from the cache with inputs.\n",
    "\n",
    "https://www.anthropic.com/pricing#api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d960dd",
   "metadata": {},
   "source": [
    "## Gemini supports both 'implicit' and 'explicit' prompt caching\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/caching?lang=python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    response = openai.chat.completions.create(model=gpt_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = [{\"role\": \"system\", \"content\": claude_system}]\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    response = anthropic.chat.completions.create(model=claude_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "display(Markdown(f\"### GPT:\\n{gpt_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### Claude:\\n{claude_messages[0]}\\n\"))\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    display(Markdown(f\"### GPT:\\n{gpt_next}\\n\"))\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    display(Markdown(f\"### Claude:\\n{claude_next}\\n\"))\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "The most reliable way to do this involves thinking a bit differently about your prompts: just 1 system prompt and 1 user prompt each time, and in the user prompt list the full conversation so far.\n",
    "\n",
    "Something like:\n",
    "\n",
    "```python\n",
    "system_prompt = \"\"\"\n",
    "You are Alex, a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.\n",
    "You are in a conversation with Blake and Charlie.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "You are Alex, in conversation with Blake and Charlie.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Alex.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
